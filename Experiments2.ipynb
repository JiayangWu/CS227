{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"TS-Data path: /Users/wujiayang/Downloads/timeseries-similarity-master/data\nDataset shape: Train: (36, 150, 1), Test: (144, 150, 1)\n(36,)\n"}],"source":"import py_ts_data\nimport time\nimport random\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport math\nimport os\nfrom kshape import _sbd as SBD\nfrom auto_encoder import AutoEncoder, train_step\nfrom utilities import min_max, normalize, augment_data, generateRandomPairs, calculatePreSBD\n\n##### settings below\ndataset = \"UMD\"\nEPOCHS = 500\nenable_data_augmentation = False\npercentage_similarity_loss = 1\nLSTM = True\n##### settings above\n\nX_train, y_train, X_test, y_test, info = py_ts_data.load_data(dataset, variables_as_channels=True)\n\nprint(\"Dataset shape: Train: {}, Test: {}\".format(X_train.shape, X_test.shape))\nprint(np.shape(y_train))\n\n\nif enable_data_augmentation or len(X_train) >= 1000:\n    # LSTM will greatly extend the training time, so disable it if we have large data\n    LSTM = False\n\n\n##### don't change two lines below\ntitle = \"Dataset:{}-DA:{}-CoefSimilar:{}-LSTM:{}\".format(dataset, enable_data_augmentation, percentage_similarity_loss, LSTM)\nenable_same_noise = False"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"X shape:  (36, 150, 1)\nY shape:  (36, 150, 1)\ndistance shape:  (36,)\n"}],"source":"num_train = len(X_train)\nif num_train < 1000 and enable_data_augmentation:\n    X_train= augment_data(X_train, enable_same_noise = enable_same_noise)\n    num_train = len(X_train)\n\n# randomly generate N pairs:\nnum_of_pairs = num_train \nX, Y = generateRandomPairs(num_of_pairs, X_train)\n# NlogN is too large, for N = 1000, NlogN would be 10K\n\nnormalized_X, normalized_Y, distance = calculatePreSBD(X, Y)\n"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"# fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n# axs[0].plot(X_train[0])\n# X_train = min_max(X_train, feature_range=(-1, 1))\n# axs[1].plot(X_train[0])\n# X_test = min_max(X_test, feature_range=(-1, 1))\n# plt.show()"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Encode and Decode"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"kwargs = {\n    \"input_shape\": (X_train.shape[1], X_train.shape[2]),\n    \"filters\": [32, 64, 128],\n    \"kernel_sizes\": [5, 5, 5],\n    \"code_size\": 16,\n}\n\nae = AutoEncoder(**kwargs)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Training"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"\nrec_loss: tf.Tensor(1.9999809, shape=(), dtype=float32) simi_loss: tf.Tensor([0.49564898], shape=(1,), dtype=float32)\nEpoch 0: [2.4956298]\nrec_loss: tf.Tensor(1.9992974, shape=(), dtype=float32) simi_loss: tf.Tensor([0.48819128], shape=(1,), dtype=float32)\nEpoch 1: [2.4874887]\nrec_loss: tf.Tensor(1.9985528, shape=(), dtype=float32) simi_loss: tf.Tensor([0.48018265], shape=(1,), dtype=float32)\nEpoch 2: [2.4787354]\nrec_loss: tf.Tensor(1.9977152, shape=(), dtype=float32) simi_loss: tf.Tensor([0.47148344], shape=(1,), dtype=float32)\nEpoch 3: [2.4691987]\nrec_loss: tf.Tensor(1.9967697, shape=(), dtype=float32) simi_loss: tf.Tensor([0.46261448], shape=(1,), dtype=float32)\n"}],"source":"loss_history = []\nt1 = time.time()\nfor epoch in range(EPOCHS):\n    #total_loss = train_step(X_train, ae)\n    total_loss = train_step(normalized_X, normalized_Y, distance, ae, alpha = percentage_similarity_loss, LSTM = LSTM)\n    loss_history.append(total_loss)\n    print(\"Epoch {}: {}\".format(epoch, total_loss), end=\"\\r\")\n    \nprint(\"The training time is:\", (time.time() - t1) / 60)"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# plt.xlim(left = 5, right = len(loss_history))\n","plt.xlabel(\"epoch starting from 5\")\n","plt.ylabel(\"loss\")\n","plt.title(\"Loss vs epoch\")\n","# print(loss_history[5:])\n","plt.plot(loss_history[5:])\n","# plt.show()\n","if not os.path.isdir(\"./images/\" + dataset):\n","    os.mkdir(\"./images/\" + dataset)\n","\n","plt.savefig(\"./images/\" + dataset + \"/\" + title + \"-loss.png\")\n"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Test"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Evaluate reconstruction"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"code_test = ae.encode(X_test, LSTM = LSTM)\ndecoded_test = ae.decode(code_test)\nplt.clf()\nplt.plot(X_test[0], label = \"Original TS\")\nplt.plot(decoded_test[0], label = \"reconstructed TS\")\n\nplt.savefig(\"./images/\" + dataset + \"/\" + title + \"-reconstruction.png\")\nplt.show()\n\nlosses = []\nfor ground, predict in zip(X_test, decoded_test):\n    losses.append(np.linalg.norm(ground - predict))\n\nL2_distance = np.array(losses).mean()\nprint(\"Mean L2 distance: {}\".format(L2_distance))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Evaluate Similarity"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"from sklearn.neighbors import NearestNeighbors\n\nnn_x_test = np.squeeze(X_test)\nbaseline_nn = NearestNeighbors(n_neighbors=10, metric=SBD).fit(nn_x_test)\ncode_nn = NearestNeighbors(n_neighbors=10).fit(code_test)# the default metric is euclidean distance\n\n# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\nbaseline_11nn = baseline_nn.kneighbors(nn_x_test, 11, return_distance=False)\ncode_11nn     = code_nn.kneighbors(code_test, 11, return_distance=False)\n\n# On average, how many common items are in the 10nn?\nresult = []\nfor b, c in zip(baseline_11nn, code_11nn):\n    # remove the first nn (itself)\n    b = set(b[1:])\n    c = set(c[1:])\n    result.append(len(b.intersection(c)))\n\nten_nn_score = np.array(result).mean()\nprint(\"10-nn score is:\", ten_nn_score)\nwith open(\"./images/\" + dataset + \"/\" + title + \"-record.txt\", \"w\") as f:\n    f.write(\" \".join([str(round(L2_distance,2)), str(round(ten_nn_score,2))]))\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}